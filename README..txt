Impression Generation Using LLM
Overview
This project aims to generate impressions based on report data using a fine-tuned Large Language Model (LLM). It utilizes the Hugging Face Transformers library and various NLP techniques to preprocess text, train the model, evaluate its performance, and visualize results.


Requirements:
Python 3.9
transformers library
datasets library
nltk for natural language processing
gensim for Word2Vec
matplotlib for visualization
Other standard libraries (e.g., os)
You can install the required packages using pip:


Code Explanation:


1. Imports and Environment Setup
The code begins by importing necessary libraries and setting the Hugging Face token for model access:

python:
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
import nltk
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from nltk.corpus import stopwords


os.environ["HUGGINGFACE_HUB_TOKEN"] = "your_token_here"


2. Loading the Dataset
The dataset is loaded using the load_dataset function from the datasets library:


Copy code
dataset = load_dataset('csv', data_files='impression_300_llm.csv')
The dataset contains columns like Report Name, History, Observation, and Impression.


3. Data Preprocessing
Data preprocessing involves creating model inputs by combining relevant fields, tokenizing, and preparing labels:



def preprocess_function(examples):
    inputs = [" ".join([report, history, observation]) for report, history, observation in zip(
        examples['Report Name'], examples['History'], examples['Observation'])]
    targets = examples['Impression']

    model_inputs = tokenizer(inputs, max_length=128, padding="max_length", truncation=True)
    labels = tokenizer(targets, max_length=128, padding="max_length", truncation=True)["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs


4. Model Fine-tuning
The model is fine-tuned using the Trainer API, configured with various training parameters:


training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
)

trainer.train()
5. Model Evaluation
After training, the model is evaluated to compute metrics like perplexity and ROUGE scores:


eval_results = trainer.evaluate()
print(eval_results)


6. Text Analysis
Text analysis is conducted using NLTK to preprocess text further:


def preprocess_text(text):
    words = nltk.word_tokenize(text)
    words = [word.lower() for word in words if word not in stop_words]
    stemmer = nltk.PorterStemmer()
    lemmatizer = nltk.WordNetLemmatizer()
    words = [stemmer.stem(word) for word in words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return words

7. Visualization
Finally, the top word pairs based on similarity are visualized using Matplotlib:


plt.figure(figsize=(10, 8))
plt.scatter(x, y)
for i, word in enumerate(word_pairs):
    plt.annotate(word[0], (x[i], y[i]))
plt.title('Top Word Pairs Based on Similarity')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.grid()
plt.show()

How to Run the Code
Make sure you have all the dependencies installed.
Update the Hugging Face token in the code.
Run the script using:
Copy code
python 5cnetwork.py
Review the output and visualizations generated by the code.


License
This project is licensed under the MIT License - see the LICENSE file for details.